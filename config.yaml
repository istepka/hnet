# config.yaml
# Hydra configuration file for train.py

defaults:
  - _self_
  - override hydra/job_logging: disabled # No fluff
  - override hydra/hydra_logging: disabled # No fluff

hydra:
  run:
    dir: ./outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}

paths:
  project_root: /home/${oc.env:USER}/hnet
  scratch_dir: /home/scratch/${oc.env:USER}
  data_dir: /data/user_data/${oc.env:USER}/data/hnet
  checkpoint_save_path: /data/user_data/${oc.env:USER}/checkpoints
  experiment_dir: ${paths.data_dir}/experiments
  model_cache: ${paths.scratch_dir}/model_cache
  hf_cache: /data/hf_cache

# name: hnet_${data.name}_${model.stages}stage_${model.size}
name: debug_ts
seed: 67
wandb: 
  use: true
  tags: ["${data.input_len}", "${data.name}", "${model.stages}-stage", "${model.size}", "${model.vocab_size}"]

model:
  stages: 1
  size: XXS
  config_path: "configs/hnet_${model.stages}stage_${model.size}.json"
  tokenizer: ts_quantized
  vocab_size: 256

data:
  name: tslib
  input_len: 256 # Input sequence length / context size
  batch_size: 8 # Batch size for training, 128 x 256 fits on L40s GPU 
  num_workers: 2
  pin_memory: false
  prefetch_factor: null
  shuffle: true
  regenerate_tokens: true
  # ag_news:
  #   path: ag_news 
  #   name: null
  # wikitext:
  #   path: Salesforce/wikitext
  #   name: wikitext-103-v1
  #   streaming: Fals
  c4:
    path: allenai/c4 # hf data main path
    name: en # hf data subset
    type: text # data type: text or timeseries
    streaming: True # if streaming then we use take_rows
    train:
      take_rows: 2000000 # Take 2M rows
    valid:
      take_rows: 10000 # Take 10k rows
  tslib:
    path: thuml/Time-Series-Library
    name: ETTm2 # weather dataset 
    target: OT
    type: timeseries
    streaming: False
    split_manually_frac: 0.9  # Manually split 90% train, 10% valid



optimizer:
  base_lr: 1e-4 # 6.25e-4 
  weight_decay: 0.1
  lr_multipliers: [2.0, 1.5, 1.0]   # [Stage 0, Stage 1, Main Network]

training:
  device: cuda
  total_steps: 300 # Total training steps (batches)
  log_every: 50 # Log training metrics every N steps
  alpha: 0.03 # Ratio loss weight
  n_ratios: [8, 3]  # N ratios for [Stage 0, Stage 1]
  save_every: 100  # Save checkpoint every N steps
  val_every: 50  # Validate every N steps

plotting:
  plot_sentences: 
    - The quick brown fox jumps over the lazy dog.
    - Wikipedia is a free online encyclopedia that aims to provide.
    - Schenley Park is a great place for a walk in Pittsburgh.
    - Artificial intelligence is transforming the world in many ways.
    - kabksbklbfieugf uiahefwuhl aleuhu evjhl hiseualghfiueigu
    - ababababababababababababababababababababababababababa
    - ab ab ab ab ab ab ab ab ab ab ab ab ab ab ab ab ab ab 
    - aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
  time_series:
    batches_to_plot: 2