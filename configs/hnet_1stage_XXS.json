{
    "arch_layout": ["m4", ["T22"], "m4"],
    "d_model": [256, 256],
    "d_intermediate": [0, 512],
    "vocab_size": 256,
    "ssm_cfg": {
        "chunk_size": 256,
        "d_conv": 4,
        "d_state": 128,
        "expand": 2
    },
    "attn_cfg": {
        "num_heads": [8, 8],
        "rotary_emb_dim": [32, 32],
        "window_size": [1023, -1]
    },
    "tie_embeddings": false
}